{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--cFi0bdP4js"
   },
   "outputs": [],
   "source": [
    "#  Import essential packages \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.datasets import load_files\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1CYgbmFP4nx"
   },
   "outputs": [],
   "source": [
    "# Implementing pretrained word embeddings\n",
    "\n",
    "\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "glove_file = datapath(os.path.abspath('glove.6B.50d.txt'))\n",
    "tmp_file = get_tmpfile(os.path.abspath(\"test_word2vec.txt\"))\n",
    "converted_file = glove2word2vec(glove_file, tmp_file) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEyjwJkxP4n8"
   },
   "outputs": [],
   "source": [
    "# Loading the Glove embeddings in word2vec format \n",
    "\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(os.path.abspath(\"test_word2vec.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "dGWXe7cUP4oA",
    "outputId": "8c078027-3382-4668-c75a-fe4b04031782",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model[\"the\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6t6QzSa9UP1j"
   },
   "outputs": [],
   "source": [
    "# Stop word removal function\n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def stop_word_remove(sentence):\n",
    "    temp = [token for token in sentence.split() if token not in STOP_WORDS]\n",
    "    return ' '.join(word for word in temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vFE3uAfZu14"
   },
   "outputs": [],
   "source": [
    "# Function to read the papers from their paths \n",
    "\n",
    "\n",
    "def read_paper(path):\n",
    "  f = open(path, 'r', encoding=\"utf-8\")\n",
    "  text = str(f.read())\n",
    "  f.close()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(text):\n",
    "\n",
    "  # Removes unwanted characters, accounting for unicode characters\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "\n",
    "    # Extracting the highlights, body from the paper\n",
    "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
    "    body_main = re.findall(r'(?<=\\bABSTRACT\\b).*', text, flags=re.I)[0]\n",
    "\n",
    "    body = ' '.join(body_main.split())\n",
    "    body = body.split('.')\n",
    "  \n",
    "    # Removing sentences that are too short or too long, as they wouldn't make apt summary text\n",
    "    for i,x in enumerate(body):\n",
    "        if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
    "            body.pop(i)\n",
    "    \n",
    "    # Making a copy of the body, lowercasing body text, removing punctuations & extra spaces\n",
    "    dummy_body = []\n",
    "    for i in body:\n",
    "        i= i.lower()\n",
    "        i = re.sub('[^\\w\\s\\d\\.]','',i)   ###[^\\w\\s\\d\\.] '.'keep here\n",
    "        dummy_body.append(i)\n",
    "\n",
    "    # Making a copy of the highlights, lowercasing body text, removing punctuations & extra spaces\n",
    "    dummy_highlights = highlights.lower()\n",
    "    dummy_highlights = re.sub('[^\\w\\s\\d]','',dummy_highlights) ###[^\\w\\s\\d] '.' remove here\n",
    "    dummy_highlights = ' '.join(dummy_highlights.split())\n",
    "    \n",
    "    # Removing stop words from body & highlights\n",
    "    body_copy = []\n",
    "    for x in dummy_body:\n",
    "        body_copy.append(stop_word_remove(x))  # call to stop word function\n",
    "        \n",
    "    highlight_copy = []\n",
    "    for x in dummy_highlights.split():\n",
    "        highlight_copy.append(stop_word_remove(x)) # call to stop word function\n",
    "\n",
    "    # Combing all of the highlights into one string    \n",
    "    highlight_copy = \" \".join(sentence for sentence in highlight_copy)\n",
    "    highlight_copy = \" \".join(highlight_copy.split())\n",
    "\n",
    "    return body_main, body_copy, highlights, highlight_copy,body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_body_copy_main(path):\n",
    "  text = read_paper(path)\n",
    "  body_main, body_copy, highlights, highlight_copy, body = process_paper(text)\n",
    "  return body_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f061913ba5942ceb5ef947ef7452b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Body_Copy_Main = []\n",
    "\n",
    "paths = glob.glob(\"Parsed_Papers/*.txt\")\n",
    "for i,path in enumerate(tqdm(paths[0:20])):\n",
    "    body_copy = create_body_copy_main(path)\n",
    "    Body_Copy_Main.extend(body_copy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(Body_Copy_Main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hesoGRNOdCCR"
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# Function to calculate sentence Score\n",
    "def document_score(body_copy, highlight_copy):\n",
    "  # Getting word vectors for the body\n",
    "  body_vectors = [] \n",
    "  for sent in body_copy:\n",
    "      sent_vec = [] \n",
    "      for word in sent.split():\n",
    "          if word in glove_model:\n",
    "              sent_vec.append(glove_model[word])\n",
    "          # If the word vector isn't there in the model\n",
    "          # then use the vector of the word \"Visual\"\n",
    "          else:\n",
    "              sent_vec.append(glove_model[\"visual\"])\n",
    "      body_vectors.append(sent_vec)\n",
    "\n",
    "  # Getting word vectors for the highlights\n",
    "  highlight_vectors = []\n",
    "  for word in highlight_copy.split():\n",
    "      if word in glove_model:\n",
    "          highlight_vectors.append(glove_model[word])\n",
    "      else:\n",
    "          highlight_vectors.append(glove_model[\"visual\"])\n",
    "\n",
    "  # Finding the rouge score for each sentence by counting the # of common words\n",
    "  # & dividing by length of sentence\n",
    "  doc_score = []\n",
    "  for sent in body_vectors:\n",
    "      sent_score = 0\n",
    "      for word in sent:\n",
    "          for w in highlight_vectors:\n",
    "              if (word == w).all():\n",
    "                  sent_score+=1\n",
    "      if sent_score>0: \n",
    "        doc_score.append(expit(sent_score/len(sent)))\n",
    "      else:\n",
    "        doc_score.append(0)\n",
    "  return doc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Vectorizer_Count(body_copy):\n",
    "    bow_table = vectorizer.transform(body_copy)\n",
    "    bow_table = bow_table.toarray()\n",
    "    return bow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVE8Pu39f8sh"
   },
   "outputs": [],
   "source": [
    "# Function to create datasets\n",
    "def create_data(path):\n",
    "  text = read_paper(path)\n",
    "  body_main, body_copy, highlights, highlight_copy, body = process_paper(text)\n",
    "  doc_score = document_score(body_copy, highlight_copy)\n",
    "  bow_table = Vectorizer_Count(body_copy)\n",
    "  x = bow_table       \n",
    "  y = pd.Series(doc_score)  \n",
    "  return x, y, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq-FWvFcNSH5"
   },
   "outputs": [],
   "source": [
    "# Import Stocastic Gradient Descent Regressor model\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9HS-vtJNold"
   },
   "outputs": [],
   "source": [
    "# Create a Gaussian Classifier \n",
    "\n",
    "\n",
    "Model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4949c5d7434283b9826a5519bc29aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Creating a list of all file paths & partially fitting the model \n",
    "\n",
    "paths = glob.glob(\"Parsed_Papers/*.txt\")\n",
    "for i,path in enumerate(tqdm(paths[0:20])):\n",
    "    x, y, body = create_data(path)         \n",
    "    Model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hqcKnUFtKGn"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GDAhO25wplfT",
    "outputId": "fd40c290-eadb-497b-d636-3d9ff84dd948"
   },
   "outputs": [],
   "source": [
    "# Test Data\n",
    "\n",
    "x1, y1, body = create_data(\"Parsed_Papers/S0003687016300539.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1FEQEiKqpzyB",
    "outputId": "539b34af-157e-46de-a1bd-75b56b3abb89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 9, 106, 80]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Predicting the top 4 summary sentences --- \n",
    "\n",
    "c = Model.predict(x1)\n",
    "lst = pd.Series(c)\n",
    "i = lst.nlargest(4)\n",
    "i = i.index.values.tolist()\n",
    "i # Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "sNExUdvLq3LB",
    "outputId": "3d277977-019e-424a-ecd1-6705ccb9b20d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' on stairways, the observer was also instructed to keep an eye on the wayfinder by walking behind him/her',\n",
       " ' Route complexity is a growing problem in hospitals because hospitals are expanding in size due to the increasing demand for health care, more specialized care, and more diagnostic techniques',\n",
       " ' This might imply that participants have an incomplete representation of the spatial setting, and therefore rely on the central point wayfinding strategy, meaning that they first walk towards a central point like the main entry hall or main corridors (Hlsher etal',\n",
       " ' A route efficiency ratio larger than 1 indicates that participants walked more meters than strictly necessary']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  --- Predicted summary --- \n",
    "\n",
    "summary = []\n",
    "\n",
    "for x in i:\n",
    "    summary.append(body[x])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on stairways the observer was also instructed to keep an eye on the wayfinder by walking behind himher Route complexity is a growing problem in hospitals because hospitals are expanding in size due to the increasing demand for health care more specialized care and more diagnostic techniques This might imply that participants have an incomplete representation of the spatial setting and therefore rely on the central point wayfinding strategy meaning that they first walk towards a central point like the main entry hall or main corridors Hlsher etal A route efficiency ratio larger than 1 indicates that participants walked more meters than strictly necessary'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paper_Summary = ' '.join(summary)\n",
    "Paper_Summary = ' '.join(Paper_Summary.split())\n",
    "Paper_Summary = re.sub('[^\\w\\s\\d\\.]','',Paper_Summary)\n",
    "Paper_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GTIymE4QrXVX",
    "outputId": "11f9bb1e-c9fd-42d3-a45b-7da3d4c8be3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Route complexity negatively influences wayfinding performance in terms of route efficiency and walking speed',\n",
       " ' Simulated elderly participants have higher heart rates and respiratory rates during a wayfinding task',\n",
       " ' Simulated physical ageing and route complexity do not interact on wayfinding performance and physiological outcomes',\n",
       " ' Physical ageing was simulated in an age-simulation field experiment by using gerontologic suits',\n",
       " ' A portable heart rate monitor was used to assess physiological outcomes like heart rate and respiratory rate',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Highlights - which are gold standard summary of the paper --- \n",
    "\n",
    "\n",
    "text = read_paper(\"Parsed_Papers/S0003687016300539.txt\")\n",
    "body_main,_, highlights,_,_ = process_paper(text)\n",
    "highlights = \" \".join(highlights.split()).split(\".\")\n",
    "highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extractive-Summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:dsml20_env2]",
   "language": "python",
   "name": "conda-env-dsml20_env2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "40px",
    "left": "1266px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
